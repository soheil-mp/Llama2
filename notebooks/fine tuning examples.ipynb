{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n<h1 style=\"text-align: center;\">Llama Fine-Tuning</h1>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n## Initial Setup\n\n---","metadata":{}},{"cell_type":"code","source":"# # Installation\n!pip -q install -U git+https://github.com/huggingface/transformers.git\n!pip -q install -U git+https://github.com/huggingface/peft.git\n!pip -q install -U git+https://github.com/huggingface/accelerate.git\n!pip -q install trl xformers wandb datasets einops gradio sentencepiece bitsandbytes\n!pip -q uninstall datasets -y\n!pip -q install -U datasets==2.16","metadata":{"execution":{"iopub.status.busy":"2024-02-13T09:59:10.041183Z","iopub.execute_input":"2024-02-13T09:59:10.041536Z","iopub.status.idle":"2024-02-13T10:03:42.990598Z","shell.execute_reply.started":"2024-02-13T09:59:10.041505Z","shell.execute_reply":"2024-02-13T10:03:42.989579Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/nvidia-cuda-nvrtc-cu12/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/nvidia-cuda-runtime-cu12/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.13 requires torch<2.2,>=1.10, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Login to huggingface\n!huggingface-cli login --token \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:03:50.487119Z","iopub.execute_input":"2024-02-13T10:03:50.488065Z","iopub.status.idle":"2024-02-13T10:03:52.321956Z","shell.execute_reply.started":"2024-02-13T10:03:50.488012Z","shell.execute_reply":"2024-02-13T10:03:52.321051Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the libraries\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb, platform, gradio, warnings\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom huggingface_hub import notebook_login","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:03:52.323706Z","iopub.execute_input":"2024-02-13T10:03:52.323985Z","iopub.status.idle":"2024-02-13T10:04:16.616108Z","shell.execute_reply.started":"2024-02-13T10:03:52.323960Z","shell.execute_reply":"2024-02-13T10:04:16.615305Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-02-13 10:03:59.214168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-13 10:03:59.214309: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-13 10:03:59.347771: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Pre trained model\nmodel_name = \"meta-llama/Llama-2-7b-hf\" \n\n# Dataset name\ndataset_name = \"vicgalle/alpaca-gpt4\"\n\n# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\nnew_model = \"soheill/Llama-2-7b-hf\"","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:04:16.617611Z","iopub.execute_input":"2024-02-13T10:04:16.618364Z","iopub.status.idle":"2024-02-13T10:04:16.622500Z","shell.execute_reply.started":"2024-02-13T10:04:16.618336Z","shell.execute_reply":"2024-02-13T10:04:16.621610Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n## Load Dataset\n\n---\n\nThis code loads a dataset from Hugging Face's vicgalle/alpaca-gpt4 and displays the first entry's 'instruction', 'input', 'output', and 'text' fields. It's a quick way to understand the dataset's structure, useful for machine learning and data analysis.\n","metadata":{}},{"cell_type":"code","source":"# Load dataset \ndataset = load_dataset(dataset_name, split=\"train[:10000]\")\n\n# Report\nprint(\"Dataset shape: \", dataset.shape, \"\\n\", \"-\"*100)\nindex = 0\nprint(\"Instruction: \\n\", dataset[\"instruction\"][index], \"\\n\", \"-\"*100)\nprint(\"Input: \\n\", dataset[\"input\"][index], \"\\n\", \"-\"*100)\nprint(\"Output: \\n\", dataset[\"output\"][index], \"\\n\", \"-\"*100)\nprint(\"Text: \\n\", dataset[\"text\"][index], \"\\n\", \"-\"*100)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:10:22.593132Z","iopub.execute_input":"2024-02-13T10:10:22.593497Z","iopub.status.idle":"2024-02-13T10:10:27.589570Z","shell.execute_reply.started":"2024-02-13T10:10:22.593468Z","shell.execute_reply":"2024-02-13T10:10:27.588553Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b9704b426c949d8b7effe5cf2022e48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/48.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d684108f88464367b02d3a2de1c9d30a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50942dbe2962408c87eb8f40ca6f699b"}},"metadata":{}},{"name":"stdout","text":"Dataset shape:  (10000, 4) \n ----------------------------------------------------------------------------------------------------\nInstruction: \n Give three tips for staying healthy. \n ----------------------------------------------------------------------------------------------------\nInput: \n  \n ----------------------------------------------------------------------------------------------------\nOutput: \n 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night. \n ----------------------------------------------------------------------------------------------------\nText: \n Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night. \n ----------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<br>\n\n## Configure 4-Bit Quantization for Efficient Training\n\n---\n\nThis code snippet initializes a configuration for 4-bit quantization during training using the BitsAndBytes library, which is aimed at optimizing memory usage and computational efficiency. The configuration (bnb_config) is set up with the following parameters:\n\n- load_in_4bit=True: This enables loading the model weights in 4-bit precision, reducing the memory footprint.\n- bnb_4bit_quant_type=\"nf4\": It specifies the type of quantization as \"nf4\", which is a specific method or format for 4-bit quantization.\n- bnb_4bit_compute_dtype=torch.float16: Sets the data type for computation to half-precision floating-point (float16), which strikes a balance between precision and performance.\n- bnb_4bit_use_double_quant=False: Disables double quantization, which means it avoids further reducing precision beyond 4-bit.\n\nThe configuration is then stored in bnb_config, which can be used in training models to enhance efficiency.","metadata":{}},{"cell_type":"code","source":"# Initialize the configuration to sets up 4-bit quantization for training, optimizing memory and computational efficiency.\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,                     \n    bnb_4bit_quant_type= \"nf4\",             \n    bnb_4bit_compute_dtype= torch.float16,  \n    bnb_4bit_use_double_quant= False,       \n)\n\nbnb_config","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:10:28.466249Z","iopub.execute_input":"2024-02-13T10:10:28.466609Z","iopub.status.idle":"2024-02-13T10:10:28.476573Z","shell.execute_reply.started":"2024-02-13T10:10:28.466576Z","shell.execute_reply":"2024-02-13T10:10:28.475687Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"BitsAndBytesConfig {\n  \"_load_in_4bit\": true,\n  \"_load_in_8bit\": false,\n  \"bnb_4bit_compute_dtype\": \"float16\",\n  \"bnb_4bit_quant_type\": \"nf4\",\n  \"bnb_4bit_use_double_quant\": false,\n  \"llm_int8_enable_fp32_cpu_offload\": false,\n  \"llm_int8_has_fp16_weight\": false,\n  \"llm_int8_skip_modules\": null,\n  \"llm_int8_threshold\": 6.0,\n  \"load_in_4bit\": true,\n  \"load_in_8bit\": false,\n  \"quant_method\": \"bitsandbytes\"\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"<br>\n\n## Llama Model\n\n---\n\nThis code snippet involves setting up a large language model (specifically 'llama-2-7b-hf') with customized training configurations for efficiency and resource management. First, it loads the base model with a specified quantization configuration (defined earlier as bnb_config) and assigns it to run on GPU 0. Then, it prepares the model for k-bit training, which might involve additional quantization or optimization steps. To reduce memory usage, particularly important for large models, caching is disabled. Finally, the tensor parallelism degree for pretraining is set to 1, a setting that can be adjusted for multi-GPU setups or distributed training scenarios. This setup is crucial for efficiently training large-scale models, especially in resource-constrained environments.","metadata":{}},{"cell_type":"code","source":"# Load the base model (llama-2-7b-hf) with the specified quantization configuration and set it to run on GPU 0\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    # device_map={\"\": 0}\n)\n\n# Prepare the model for k-bit training (possibly setting up additional quantization or optimization parameters)\nmodel = prepare_model_for_kbit_training(model)\n\n# Disable caching to reduce memory usage during training; useful for large models\nmodel.config.use_cache = False \n\n# Set the tensor parallelism degree for pretraining to 1 (could be adjusting for multi-GPU setups or distributed training)\nmodel.config.pretraining_tp = 1\n\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:10:29.358697Z","iopub.execute_input":"2024-02-13T10:10:29.359525Z","iopub.status.idle":"2024-02-13T10:11:35.417355Z","shell.execute_reply.started":"2024-02-13T10:10:29.359492Z","shell.execute_reply":"2024-02-13T10:11:35.416354Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7303f883a228445ca4bad5bb71737540"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ebfdfe018b4f9c9323010ce9242847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2771ae4ee0c24ba19c01961eab8750fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fbe6bfcdee4967914ced4ecc58d269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d01e523e1844199a831bcc6a9104940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd1359b43f24dd7928b8711c619ac19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eddd0edb24b486eb50412a59a7be2c3"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"<br>\n\n## Tokenizer\n\n---\n\nThis code initializes a tokenizer for the LLaMA model and configures its behavior for sequence processing. It loads the tokenizer using AutoTokenizer.from_pretrained, allowing remote code execution if necessary for initialization. The padding token is set to be the same as the end-of-sequence (EOS) token, ensuring consistency in padding and sequence termination. Additionally, the tokenizer is configured to add both a beginning-of-sequence (BOS) and an end-of-sequence (EOS) token to each sequence, facilitating clear demarcation of the start and end of text inputs. This setup is important for models dealing with varied and potentially complex text data, as it helps maintain sequence integrity and context understanding.","metadata":{}},{"cell_type":"code","source":"# Load the LLaMA tokenizer, allowing execution of remote code if necessary for initialization\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Set the padding token of the tokenizer to be the same as the end-of-sequence token\ntokenizer.pad_token = tokenizer.eos_token\n\n# Enable the addition of an end-of-sequence token to each sequence by the tokenizer\ntokenizer.add_eos_token = True\n\n# Set the tokenizer to add a beginning-of-sequence and an end-of-sequence token to each sequence\ntokenizer.add_bos_token, tokenizer.add_eos_token = True, True\n\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:11:35.419143Z","iopub.execute_input":"2024-02-13T10:11:35.419442Z","iopub.status.idle":"2024-02-13T10:11:37.341209Z","shell.execute_reply.started":"2024-02-13T10:11:35.419415Z","shell.execute_reply":"2024-02-13T10:11:37.340261Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9455c375d0714fba9e722fce4b3958cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c57af7671c54ff783f66e8c38738972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27f054bf71e7492ca47a8458a0604e94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c1959786294085ac95c6dfd39bf17a"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"<br>\n\n## Weights & Biases (Monitoring)\n\n---\n\nThis code snippet is used for integrating Weights and Biases (wandb), a popular tool for experiment tracking and monitoring in machine learning workflows.\n\nFirst, it logs into Weights and Biases using a specific API key, which is essential for authentication and accessing the wandb services.\nThen, it initializes a new wandb run, which is a single instance of a model training or evaluation process. This is done using wandb.init, where the project is named 'Fine tuning llama-2-7B', the job type is set as 'training', and the anonymity setting is 'allow', which might be related to how user data is handled or displayed.\n\nThis setup is crucial for tracking the performance, hyperparameters, and outputs of the training process, allowing for more organized and efficient machine learning experiments.","metadata":{}},{"cell_type":"code","source":"# Log in to Weights and Biases (wandb) for experiment tracking and monitoring, using the provided API key\nwandb.login(key=\"d5dc049e40baee0f32ab437502140a7ac844cda0\")\n\n# Initialize a Weights and Biases run for tracking and organizing the training process, specifying project name, job type, and anonymity settings\nrun = wandb.init(project='Fine tuning llama-2-7B', job_type=\"training\", anonymous=\"allow\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:11:37.342436Z","iopub.execute_input":"2024-02-13T10:11:37.342829Z","iopub.status.idle":"2024-02-13T10:12:10.695579Z","shell.execute_reply.started":"2024-02-13T10:11:37.342758Z","shell.execute_reply":"2024-02-13T10:12:10.694646Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoheil-mpg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240213_101139-t7otcetp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/soheil-mpg/Fine%20tuning%20llama-2-7B/runs/t7otcetp' target=\"_blank\">enchanting-dog-6</a></strong> to <a href='https://wandb.ai/soheil-mpg/Fine%20tuning%20llama-2-7B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/soheil-mpg/Fine%20tuning%20llama-2-7B' target=\"_blank\">https://wandb.ai/soheil-mpg/Fine%20tuning%20llama-2-7B</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/soheil-mpg/Fine%20tuning%20llama-2-7B/runs/t7otcetp' target=\"_blank\">https://wandb.ai/soheil-mpg/Fine%20tuning%20llama-2-7B/runs/t7otcetp</a>"},"metadata":{}}]},{"cell_type":"markdown","source":"<br>\n\n## Fine-Tuning\n\n---\n\nThis code provides a comprehensive setup for fine-tuning a causal language model using specialized training techniques and tools. It begins by initializing the LoraConfig with parameters tailored for efficient training, targeting specific transformer modules with LoRA (Low-Rank Adaptation) adjustments. This setup aims to optimize the model's performance for causal language modeling tasks.\n\n","metadata":{}},{"cell_type":"code","source":"# Initialize the LoraConfig with specific parameters for efficient training of causal language models, targeting specific modules for optimization\npeft_config = LoraConfig(\n    lora_alpha= 8,          # Set the scaling factor alpha for LoRA (Low-Rank Adaptation)\n    lora_dropout= 0.1,      # Specify the dropout rate for LoRA layers\n    r= 16,                  # Define the rank for the low-rank matrices in LoRA\n    bias=\"none\",            # Indicate no bias to be used in LoRA layers\n    task_type=\"CAUSAL_LM\",  # Specify the task type as Causal Language Modeling\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"]    # List of transformer modules to apply LoRA\n)\n\npeft_config","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:12:10.699586Z","iopub.execute_input":"2024-02-13T10:12:10.699896Z","iopub.status.idle":"2024-02-13T10:12:10.708431Z","shell.execute_reply.started":"2024-02-13T10:12:10.699868Z","shell.execute_reply":"2024-02-13T10:12:10.707462Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'up_proj', 'o_proj', 'gate_proj', 'k_proj', 'v_proj', 'q_proj'}, lora_alpha=8, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, training arguments are defined, specifying various hyperparameters and configurations such as batch size, learning rate, optimizer type, and logging intervals. These settings are crucial for controlling the training process and ensuring efficient learning.","metadata":{}},{"cell_type":"code","source":"# Define training arguments for the model, setting various hyperparameters and training configurations\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",                 # Directory to save training outputs\n    num_train_epochs=1,                     # Number of training epochs\n    per_device_train_batch_size=4,          # Batch size per device during training\n    gradient_accumulation_steps=2,          # Number of steps for gradient accumulation\n    optim=\"paged_adamw_8bit\",               # Specify optimizer as 8-bit precision AdamW variant\n    save_steps=1000,                        # Interval for saving the model\n    logging_steps=100,                      # Interval for logging, adjusted for better monitoring\n    learning_rate=2e-4,                     # Learning rate\n    weight_decay=0.001,                     # Weight decay for regularization\n    fp16=True,                              # Enable training in 16-bit floating point precision for efficiency\n    max_grad_norm=0.3,                      # Maximum gradient norm for gradient clipping\n    warmup_ratio=0.1,                       # Adjusted warmup ratio for the learning rate\n    group_by_length=True,                   # Group samples of similar lengths together\n    lr_scheduler_type=\"linear\",             # Type of learning rate scheduler\n    report_to=\"wandb\",                      # Report training progress to Weights and Biases\n    load_best_model_at_end=True,            # Load the best model at the end of training\n    evaluation_strategy=\"steps\",            # Evaluate the model periodically\n    eval_steps=500,                         # Steps interval for evaluation\n)\n\ntraining_arguments","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:12:10.709602Z","iopub.execute_input":"2024-02-13T10:12:10.709946Z","iopub.status.idle":"2024-02-13T10:12:10.726758Z","shell.execute_reply.started":"2024-02-13T10:12:10.709919Z","shell.execute_reply":"2024-02-13T10:12:10.725811Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=500,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=2,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=False,\ngroup_by_length=True,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0002,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./results/runs/Feb13_10-12-10_cf50fa43bfd9,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=100,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=linear,\nmax_grad_norm=0.3,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=1,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=./results,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['wandb'],\nresume_from_checkpoint=None,\nrun_name=./results,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=1000,\nsave_strategy=steps,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=False,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.001,\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"The core of the code is the setup of the Sparse Fine-Tuning (SFT) Trainer, which is configured with the model, dataset, LoRA configuration, tokenizer, and training arguments. This trainer is designed to fine-tune the model effectively while managing computational resources.","metadata":{}},{"cell_type":"code","source":"# Setting up the Sparse Fine-Tuning (SFT) Trainer with model, dataset, and training configurations\ntrainer = SFTTrainer(\n    model=model,                         # The model to be trained\n    train_dataset=dataset,               # The dataset used for training\n    peft_config=peft_config,             # Configuration for Low-Rank Adaptation (LoRA) fine-tuning\n    max_seq_length=512,                  # Set a reasonable max sequence length limit\n    dataset_text_field=\"text\",           # The field in the dataset containing text data\n    tokenizer=tokenizer,                 # The tokenizer for processing text data\n    args=training_arguments,             # Training arguments including hyperparameters and settings\n    packing=True,                        # Enable packing for optimized memory and speed\n)\n\ntrainer","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:12:10.727920Z","iopub.execute_input":"2024-02-13T10:12:10.728283Z","iopub.status.idle":"2024-02-13T10:12:17.018734Z","shell.execute_reply.started":"2024-02-13T10:12:10.728257Z","shell.execute_reply":"2024-02-13T10:12:17.017851Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9725d6a9134285bd4d165f530d5bdb"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7d78718a0a30>"},"metadata":{}}]},{"cell_type":"markdown","source":"The training process is then executed, followed by saving the fine-tuned model. Additionally, the script integrates Weights and Biases (wandb) for tracking and monitoring the training process, including a clean shutdown of the wandb run after training. The code re-enables caching for faster inference and sets the model to evaluation mode, disabling training-specific behaviors like dropout. This comprehensive approach is aimed at achieving an optimized and well-monitored training process for a causal language model.\n","metadata":{}},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T10:12:17.020011Z","iopub.execute_input":"2024-02-13T10:12:17.020381Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='164' max='538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [164/538 1:08:25 < 2:37:58, 0.04 it/s, Epoch 0.30/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model to the specified directory 'new_model'\ntrainer.model.save_pretrained(new_model)\n\n# Finish and close the current Weights and Biases (wandb) run, stopping all logging and tracking\nwandb.finish()\n\n# Re-enable caching for the model, which can speed up inference by storing and reusing certain computations\nmodel.config.use_cache = True\n\n# Set the model to evaluation mode, which disables training-specific behaviors like dropout\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n## Generation\n\n---","metadata":{}},{"cell_type":"code","source":"dataset[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_response(user_prompt):\n    \n    # Set the device for model execution to GPU 0\n    runtimeFlag = \"cuda:0\"  \n    \n    # Prompt\n    prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{user_prompt.strip()}\\n\\n### Response:\\n\"\n\n    # Tokenize the prompt and move the tensors to the specified runtime device (GPU)\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n\n    # Initialize a TextStreamer object to handle the text streaming functionality\n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    # Generate the response using the model, passing in the inputs and streamer, with a limit on new tokens\n    # This will also print the generated text to stdout\n    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T16:43:23.035907Z","iopub.status.idle":"2024-02-12T16:43:23.036428Z","shell.execute_reply.started":"2024-02-12T16:43:23.036138Z","shell.execute_reply":"2024-02-12T16:43:23.036180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_response(\"What are the three primary colors?\")","metadata":{"execution":{"iopub.status.busy":"2024-02-12T16:43:23.037569Z","iopub.status.idle":"2024-02-12T16:43:23.038063Z","shell.execute_reply.started":"2024-02-12T16:43:23.037805Z","shell.execute_reply":"2024-02-12T16:43:23.037827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n## Loading the Pre-Trained Model\n\n---","metadata":{}},{"cell_type":"code","source":"# Clear the memory footprint\ndel model, trainer\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize and load the base causal language model with specific configurations for memory and computational efficiency\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    low_cpu_mem_usage=True,      # Optimize CPU memory usage, especially useful for large models\n    return_dict=True,           # Ensure the output is returned as a PyTorch dict object for easier manipulation\n    torch_dtype=torch.float16,  # Set model to use 16-bit floating point precision to reduce memory usage and potentially increase performance\n    device_map= {\"\": 0})        # Allocate the model to GPU 0 for faster computation\n\n# Load a pre-trained PEFT (Progressive Embedding Fine-Tuning) model with custom configurations, using the base model as a starting point\nmodel = PeftModel.from_pretrained(base_model, new_model)\n\n# Optimize the model by merging fragmented tensors and unloading unnecessary parts from the GPU to free up memory\nmodel = model.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n## Push to Hugging Face\n\n---","metadata":{}},{"cell_type":"code","source":"model.push_to_hub(new_model)\ntokenizer.push_to_hub(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T16:43:23.040385Z","iopub.status.idle":"2024-02-12T16:43:23.041060Z","shell.execute_reply.started":"2024-02-12T16:43:23.040806Z","shell.execute_reply":"2024-02-12T16:43:23.040827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}